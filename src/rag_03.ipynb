{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from typing import Optional\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    match result.datasource.lower():\n",
    "        case \"python_docs\":\n",
    "            return \"chain for python_docs\"\n",
    "        case \"js_docs\":\n",
    "            return \"chain for js_docs\"\n",
    "        case _:\n",
    "            return \"chain for golang_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python_docs'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using teacher:  Physics Idx:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am a hands-on physics teacher who loves using real-world examples to explain concepts. Gravity is the force that pulls objects towards each other. This force is responsible for keeping us grounded on the Earth and for the movement of planets around the sun. \\n\\nImagine you are standing on a rooftop and you drop a ball. The ball falls to the ground because of gravity pulling it towards the Earth. Gravity is what makes us feel heavier when we are on a planet with a larger mass, like Jupiter, and lighter when we are on a planet with a smaller mass, like Mars. Gravity is a fundamental force in the universe that affects everything around us.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_template = \"\"\" You are a smart math teacher and explain every concept in very detail. Please answer the question: {question} and state what kind of teacher you are: \"\"\"\n",
    "physics_template = \"\"\" You are a smart physics teacher and explain every concept by examples in the real world. Please answer the question: {question} and state what kind of teacher you are: \"\"\"\n",
    "\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "def logical_router(query):\n",
    "    prompt_embedding = embeddings.embed_query(query[\"question\"])\n",
    "    templates = [math_template, physics_template]\n",
    "    template_embeddings = embeddings.embed_documents(templates)\n",
    "    teacher_idx = cosine_similarity([prompt_embedding], template_embeddings).argmax()\n",
    "    print(\"Using teacher: \", \"Physics\" if teacher_idx else \"Math\", \"Idx: \", teacher_idx)\n",
    "    return PromptTemplate.from_template(templates[teacher_idx])\n",
    "\n",
    "routed_chain = {\"question\": RunnablePassthrough()} | RunnableLambda(logical_router) | ChatOpenAI() | StrOutputParser()\n",
    "routed_chain.invoke(\"What is gravity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using teacher:  Math Idx:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I am a secondary math teacher with a passion for exploring the deep mathematical concepts behind the numbers we use every day. In terms of the mathematical background of pi, let's delve into this fascinating topic.\\n\\nPi, denoted by the Greek letter Ï€, is a mathematical constant that represents the ratio of a circle's circumference to its diameter. It is an irrational number, meaning that it cannot be expressed as a simple fraction and its decimal representation goes on infinitely without repeating.\\n\\nHistorically, the concept of pi has been studied and used in mathematics for thousands of years. The ancient Egyptians and Babylonians both approximated pi to be around 3.125, while the ancient Greek mathematician Archimedes used inscribed and circumscribed polygons to approximate pi as 22/7.\\n\\nIn modern mathematics, pi is commonly approximated as 3.14159, but its exact value is infinitely complex and cannot be represented precisely in decimal form. It is a transcendental number, meaning it is not the root of any non-zero polynomial equation with rational coefficients.\\n\\nPi appears in various areas of mathematics, physics, and engineering, such as in trigonometry, calculus, and geometry. It plays a crucial role in formulas for calculating areas, volumes, and circumferences of shapes, as well as in understanding the behavior of waves, oscillations, and probabilities.\\n\\nIn conclusion, the mathematical background of pi is rich and multifaceted, with connections to a wide range of mathematical disciplines. Its irrationality and transcendence make it a fascinating and fundamental constant in the mathematical world.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routed_chain.invoke(\"What is the mathematical background of PI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "from yt_dlp import YoutubeDL\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import json\n",
    "\n",
    "\n",
    "def get_video_details(video_url):\n",
    "    \"\"\"Extract video details using yt-dlp.\"\"\"\n",
    "    with YoutubeDL() as ydl:\n",
    "        info = ydl.extract_info(video_url, download=False)\n",
    "    return info\n",
    "\n",
    "def get_video_transcript(video_id, lang=\"en\"):\n",
    "    \"\"\"Fetch transcript using YouTube Transcript API.\"\"\"\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang])\n",
    "        return transcript  # Returns a list of dicts: [{'text': ..., 'start': ..., 'duration': ...}, ...]\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Extract video details\n",
    "# video_info = get_video_details(VIDEO_URL)\n",
    "\n",
    "# # Extract transcript\n",
    "# video_id = video_info.get(\"id\")  # Get video ID\n",
    "# transcript = get_video_transcript(video_id)\n",
    "\n",
    "# # Print results\n",
    "# print(json.dumps(video_info, indent=2))  # Video details\n",
    "# print(json.dumps(transcript, indent=2))  # Transcript\n",
    "\n",
    "\n",
    "docs = get_video_details(\"https://www.youtube.com/watch?v=pPStdjuYzSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(docs, indent=2))  # Video details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kicke\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kicke\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1363: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: rag from scratch\n",
      "title_search: rag\n",
      "min_length_sec: 0\n",
      "max_length_sec: 600\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "query_analyzer = prompt | structured_llm\n",
    "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: modern LLM architectures trending\n",
      "title_search: 2025\n",
      "min_view_count: 10000\n",
      "earliest_publish_date: 2025-01-01\n",
      "latest_publish_date: 2026-01-01\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke({\"question\": \"Videos from year 2025 about modern LLM architectures that are trending and watched many times\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
